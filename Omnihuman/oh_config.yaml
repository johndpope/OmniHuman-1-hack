# OmniHuman Trainer Configuration

# Training parameters
learning_rate: 1e-4
total_steps: 100000
batch_size: 4
num_frames: 16
num_workers: 4
log_interval: 100
gradient_accumulation_steps: 2

# Model parameters
model:
  hidden_dim: 512
  n_heads: 8
  n_layers: 12
  dropout: 0.1
  frame_size: [256, 256]
  heatmap_size: [64, 64]

# Optimization
optimizer:
  type: "adamw"
  betas: [0.9, 0.999]
  weight_decay: 0.01
  eps: 1e-8

# Scheduler
scheduler:
  type: "cosine"
  warmup_steps: 1000

# Mixed precision
mixed_precision: "fp16"  # Options: "no", "fp16", "bf16"

# Data parameters
data:
  data_dir: "data/omnihuman"
  frame_size: [256, 256]
  num_keypoints: 33
  heatmap_size: [64, 64]
  sigma: 2.0

# Stage configurations
stages:
  stage1:
    condition_ratios:
      text: 1.0
      reference: 1.0
      audio: 0.0
      pose: 0.0
  stage2:
    condition_ratios:
      text: 1.0
      reference: 1.0
      audio: 0.5
      pose: 0.0
  stage3:
    condition_ratios:
      text: 1.0
      reference: 1.0
      audio: 0.25
      pose: 0.13

# Logging and checkpoints
output_dir: "outputs/omnihuman_run1"
checkpoint_frequency: 5000
eval_frequency: 1000

# Wandb configuration
use_wandb: true
wandb_project: "OmniHuman"
wandb_group: "training"
run_name: "flow_matching_v1"

# Distributed training
num_nodes: 1